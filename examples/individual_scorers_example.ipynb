{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35db3f2d",
   "metadata": {},
   "source": [
    "# Individual Scorers Example\n",
    "\n",
    "In this notebook, we show how to use each of the scorers in `tvalmetrics.scorers`.\n",
    "\n",
    "First we set up a RAG system using llama index on our 6 founders Paul Graham essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07786f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./paul_graham_essays\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60795f9",
   "metadata": {},
   "source": [
    "Have a simple sample question and reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What makes Sam Altman a good founder?\"\n",
    "reference_answer = \"He has a great force of will.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a088c",
   "metadata": {},
   "source": [
    "Get the response and retrieved context from the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48024bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a9c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam Altman is considered a good founder because he possesses qualities that are highly valued in the startup world. He is known for his determination and force of will, which are crucial for overcoming obstacles and not getting demoralized easily. Altman also demonstrates flexibility, being able to modify his dreams and adapt to the unpredictable nature of startups. Additionally, he has a strong imagination, which allows him to come up with surprising and innovative ideas. Altman's naughtiness or willingness to break rules that don't matter is seen as a positive trait, as it shows his willingness to think outside the box and challenge the status quo. Lastly, Altman's ability to build strong relationships and work well with others, such as his close friendship with Emmett Shear, is also seen as a valuable quality in a founder.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6303514",
   "metadata": {},
   "source": [
    "Load the answer and retrieved context into the variables `llm_answer` and `context_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de132f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer = response.response\n",
    "retrieved_context_list = [x.node.text for x in response.source_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f1fee",
   "metadata": {},
   "source": [
    "### Answer Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b585568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import AnswerSimilarityScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = AnswerSimilarityScorer(llm_evaluator)\n",
    "\n",
    "similarity_score = scorer.score(question, reference_answer, llm_answer) # score is a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd677b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726121a",
   "metadata": {},
   "source": [
    "### Retrieval Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901769b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import RetrievalPrecisionScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = RetrievalPrecisionScorer(llm_evaluator)\n",
    "\n",
    "retrieval_precision_score = scorer.score(question, retrieved_context_list) \n",
    "\n",
    "score = retrieval_precision_score.score\n",
    "context_relevant_list = retrieval_precision_score.context_relevant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2587bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cb2cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_relevant_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47ffd0",
   "metadata": {},
   "source": [
    "### Augmentation Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6de15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import AugmentationPrecisionScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = AugmentationPrecisionScorer(llm_evaluator)\n",
    "\n",
    "# AugmentationPrecisionScore object\n",
    "augmentation_precision_score = scorer.score(question, llm_answer, retrieved_context_list)\n",
    "\n",
    "# float between 0 and 1\n",
    "score = augmentation_precision_score.score\n",
    "\n",
    "# list of bools of whether each context in retrieved_context_list is relevant\n",
    "context_relevant_list = augmentation_precision_score.context_relevant_list\n",
    "\n",
    "# list of bools of whether each context in context_relevant_list appears in llm_answer\n",
    "answer_contains_context_list = augmentation_precision_score.answer_contains_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "628249fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e67f3ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_relevant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818f7c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_contains_context_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edee6e",
   "metadata": {},
   "source": [
    "### Augmentation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fed4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import AugmentationAccuracyScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = AugmentationAccuracyScorer(llm_evaluator)\n",
    "\n",
    "# AugmentationAccuracyScore object\n",
    "augmentation_accuracy_score = scorer.score(llm_answer, retrieved_context_list)\n",
    "\n",
    "# float between 0 and 1\n",
    "score = augmentation_accuracy_score.score\n",
    "\n",
    "# list of bools of whether content from each context in retrieved_content_list appears in the answer\n",
    "answer_contains_context_list = augmentation_accuracy_score.answer_contains_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75ecb493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_contains_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ff5f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12311d4b",
   "metadata": {},
   "source": [
    "### Answer Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "828ca19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import AnswerConsistencyScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = AnswerConsistencyScorer(llm_evaluator)\n",
    "\n",
    "# AnswerConsistencyScore object\n",
    "answer_consistency_score = scorer.score(llm_answer, retrieved_context_list)\n",
    "\n",
    "# float between 0 and 1\n",
    "score = answer_consistency_score.score\n",
    "\n",
    "# list of main points in the answer\n",
    "main_point_list = answer_consistency_score.main_point_list\n",
    "\n",
    "# list of bools of whether each main point is derived from the context\n",
    "main_point_derived_from_context_list = answer_consistency_score.main_point_derived_from_context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd3de0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5294240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sam Altman is considered a good founder because he possesses qualities that are highly valued in the startup world.',\n",
       " 'He is known for his determination and force of will, which are crucial for overcoming obstacles and not getting demoralized easily.',\n",
       " 'Altman also demonstrates flexibility, being able to modify his dreams and adapt to the unpredictable nature of startups.',\n",
       " 'Additionally, he has a strong imagination, which allows him to come up with surprising and innovative ideas.',\n",
       " \"Altman's naughtiness or willingness to break rules that don't matter is seen as a positive trait, as it shows his willingness to think outside the box and challenge the status quo.\",\n",
       " \"Lastly, Altman's ability to build strong relationships and work well with others, such as his close friendship with Emmett Shear, is also seen as a valuable quality in a founder.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_point_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea4bf66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_point_derived_from_context_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953143c",
   "metadata": {},
   "source": [
    "### Answer Consistency Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed23e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import AnswerConsistencyBinaryScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = AnswerConsistencyBinaryScorer(llm_evaluator)\n",
    "\n",
    "# binary integer\n",
    "score = scorer.score(llm_answer, retrieved_context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98216a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2151e87",
   "metadata": {},
   "source": [
    "### Retrieval k-Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acffc8f",
   "metadata": {},
   "source": [
    "For retrieval k-recall, we set up a new RAG system where we make the retriever explicit, so that we can retrieve the top k context. We do k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f036d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "\n",
    "# retrieval k-recall with k = 5\n",
    "k = 5\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./paul_graham_essays\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "rag_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "top_k_retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=k\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=rag_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dac485",
   "metadata": {},
   "source": [
    "Get the answer, retrieved context, and top k context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ce42d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(question)\n",
    "\n",
    "answer = response.metadata\n",
    "\n",
    "retrieved_context_list = [x.node.text for x in response.source_nodes]\n",
    "\n",
    "query_bundle = QueryBundle(question)\n",
    "top_k_nodes = top_k_retriever.retrieve(query_bundle)\n",
    "top_k_retrieved_context_list = [x.node.text for x in top_k_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b42cde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvalmetrics.scorers import RetrievalKRecallScorer\n",
    "\n",
    "llm_evaluator = \"gpt-4\"\n",
    "scorer = RetrievalKRecallScorer(llm_evaluator)\n",
    "\n",
    "score = scorer.score(\n",
    "    question, retrieved_context_list, top_k_retrieved_context_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e3badc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "123daf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, False, False, False]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.top_k_context_relevant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b6a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
