{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074e394b",
   "metadata": {},
   "source": [
    "# Llama Index Quick Start Example\n",
    "\n",
    "In the spirit of [Llama Index's starter tutorial](https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html) and Andrej Karpathy's [Unreasonable Effectiveness of RNNs blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), we start with an example of a RAG system where the document set consists of Paul Graham essays. (Footnote: The Paul Graham essay text files used were derived from the dataset of Paul Graham essays found in [paul-graham-gpt](https://github.com/mckaywrigley/paul-graham-gpt) github project.)\n",
    "\n",
    "In this notebook, we set up a simple RAG system using llama index, and evaluate the RAG system using 10 predetermined questions and answers about Paul Graham essays. The Paul Graham essays that make up the document set are the 6 essays that have the word founder in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07786f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "# llama index imports\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "# tvalmetrics imports\n",
    "# scorer\n",
    "from tvalmetrics.validate_scorer import ValidateScorer \n",
    "# metrics\n",
    "from tvalmetrics.metrics.answer_consistency_metric import AnswerConsistencyMetric\n",
    "from tvalmetrics.metrics.answer_similarity_metric import AnswerSimilarityMetric\n",
    "from tvalmetrics.metrics.augmentation_accuracy_metric import AugmentationAccuracyMetric\n",
    "from tvalmetrics.metrics.augmentation_precision_metric import AugmentationPrecisionMetric\n",
    "from tvalmetrics.metrics.retrieval_precision_metric import RetrievalPrecisionMetric\n",
    "# llm utils\n",
    "from tvalmetrics.classes.llm_response import LLMResponse\n",
    "from tvalmetrics.classes.benchmark_item import BenchmarkItem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3ee69",
   "metadata": {},
   "source": [
    "Set up a simple llama index RAG system that uses the default LlamaIndex parameters. The default LlamaIndex parameters use Open AIs ada-002 embedding model as the embedder and gpt-3.5-turbo as the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c362a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./paul_graham_essays\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f08b09",
   "metadata": {},
   "source": [
    "Load 10 questions and answers about the Paul Graham essays as a benchmark for how the RAG system should answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b34a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"question_and_answer_list.json\", \"r\") as f:\n",
    "    question_and_answer_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79df11d",
   "metadata": {},
   "source": [
    "Let's inspect an example, question, answer from the RAG system and reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfedbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_q_and_a = question_and_answer_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce814d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What makes Sam Altman a good founder?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_q_and_a[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b57275",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(ex_q_and_a[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eee554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam Altman is considered a good founder because he possesses qualities that are highly valued in the startup world. These qualities include determination, flexibility, imagination, naughtiness, and the ability to build strong relationships. Sam Altman's force of will and determination make him someone who is likely to achieve his goals and overcome obstacles. He also demonstrates flexibility by being willing to modify his ideas and adapt to changing circumstances. Altman's imagination allows him to come up with innovative and surprising ideas, which is crucial in the startup world where good ideas may initially seem bad. Additionally, Altman has a piratical gleam in his eye, meaning he is not afraid to break rules that don't matter and think outside the box. Finally, Altman understands the importance of friendship and strong relationships between founders, as evidenced by his close friendship and successful working relationship with others.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac98a094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He has a great force of will.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_q_and_a[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a255c6",
   "metadata": {},
   "source": [
    "Set up the scorer from Tonic Validate. ScoreCalculator will calculate **answer similarity score**, **retrieval precision**, **augmentation precision**, **augmentation accuracy**, and **answer consistency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b585568",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    AnswerSimilarityMetric(),\n",
    "    RetrievalPrecisionMetric(),\n",
    "    AugmentationAccuracyMetric(),\n",
    "    AugmentationPrecisionMetric(),\n",
    "    AnswerConsistencyMetric()\n",
    "]\n",
    "# can use an OpenAI chat completion model\n",
    "# llm_evaluator = \"gpt-3.5-turbo\"\n",
    "llm_evaluator = \"gpt-4-1106-preview\"\n",
    "validate_scorer = ValidateScorer(\n",
    "    metrics, llm_evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8732cf-1461-489d-8055-8432519443a6",
   "metadata": {},
   "source": [
    "To test out the scorer, first we'll score the example question and answer about Sam Altman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c1b59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example BenchmarkItem\n",
    "question = ex_q_and_a[\"question\"]\n",
    "reference_answer = ex_q_and_a[\"answer\"]\n",
    "benchmark_item = BenchmarkItem(\n",
    "    question=question,\n",
    "    reference_answer=reference_answer\n",
    ")\n",
    "\n",
    "# example LLMResponse\n",
    "llm_answer = response.response\n",
    "context_list = [source_node.node.text for source_node in response.source_nodes]\n",
    "llm_response = LLMResponse(\n",
    "    llm_answer=llm_answer,\n",
    "    llm_context_list=context_list,\n",
    "    benchmark_item=benchmark_item\n",
    ")\n",
    "\n",
    "responses = [llm_response]\n",
    "\n",
    "response_scores = validate_scorer.score_run(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada86e4-65c9-4093-94c9-19045e48d089",
   "metadata": {},
   "source": [
    "Package the scores into a simple dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba6765b-4daa-403c-975f-5695725ab6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_dictionary(score_list):\n",
    "    score_dict = {}\n",
    "    for score in score_list:\n",
    "        score_dict[score.metric_name] = score.score\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0030cff-86a0-4072-a3ef-25f5b914b9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_similarity': 4.0,\n",
       " 'retrieval_precision': 1.0,\n",
       " 'augmentation_accuracy': 0.5,\n",
       " 'augmentation_precision': 0.5,\n",
       " 'answer_consistency': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict = make_score_dictionary(response_scores[0])\n",
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894c568",
   "metadata": {},
   "source": [
    "Answer all 10 questions and get scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d8ca4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for q_and_a in question_and_answer_list:\n",
    "    query_response = query_engine.query(q_and_a[\"question\"])\n",
    "\n",
    "    benchmark_item = BenchmarkItem(\n",
    "        question=q_and_a[\"question\"],\n",
    "        reference_answer=q_and_a[\"answer\"]\n",
    "    )\n",
    "\n",
    "    llm_response = LLMResponse(\n",
    "        llm_answer=query_response.response,\n",
    "        llm_context_list=[source_node.node.text for source_node in response.source_nodes],\n",
    "        benchmark_item=benchmark_item\n",
    "    )\n",
    "\n",
    "    responses.append(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062d4508-9215-4be9-b0d7-752abe6d3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scores = validate_scorer.score_run(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de9cc5-479d-46a2-a8eb-821436ac7e8d",
   "metadata": {},
   "source": [
    "Put the scores into a dataframe for easy viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5432a16-53b3-4f05-bba0-066a6fcf7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scores_df(response_scores):\n",
    "    scores_df = {\n",
    "        \"question\": [],\n",
    "        \"reference_answer\": [],\n",
    "        \"llm_answer\": [],\n",
    "        \"retrieved_context\": []\n",
    "    }\n",
    "    for score in response_scores[0]:\n",
    "        scores_df[score.metric_name] = []\n",
    "    for score_list in response_scores:\n",
    "        scores_df[\"question\"].append(score_list[0].llm_response.benchmark_item.question)\n",
    "        scores_df[\"reference_answer\"].append(score_list[0].llm_response.benchmark_item.reference_answer)\n",
    "        scores_df[\"llm_answer\"].append(score_list[0].llm_response.llm_answer)\n",
    "        scores_df[\"retrieved_context\"].append(score_list[0].llm_response.llm_context_list)\n",
    "        for score in score_list:\n",
    "            scores_df[score.metric_name].append(score.score)\n",
    "    return pd.DataFrame(scores_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "319b1c47-c816-4d77-bd33-1606f733c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = make_scores_df(response_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76d0cec9-2d08-4b7b-a4f0-9ca865a284a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>retrieval_precision</th>\n",
       "      <th>augmentation_accuracy</th>\n",
       "      <th>augmentation_precision</th>\n",
       "      <th>answer_consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What makes Sam Altman a good founder?</td>\n",
       "      <td>He has a great force of will.</td>\n",
       "      <td>Sam Altman is considered a good founder becaus...</td>\n",
       "      <td>[Five Founders\\n\\nApril 2009\\n\\nInc recently a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When was the essay \"Five Founders\" written?</td>\n",
       "      <td>April 2009</td>\n",
       "      <td>April 2009</td>\n",
       "      <td>[Five Founders\\n\\nApril 2009\\n\\nInc recently a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When does the most dramatic growth happen for ...</td>\n",
       "      <td>When the startup only has three or four people.</td>\n",
       "      <td>The most dramatic growth for a startup typical...</td>\n",
       "      <td>[Five Founders\\n\\nApril 2009\\n\\nInc recently a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the problem with business culture vers...</td>\n",
       "      <td>In business culture, energy is expended on out...</td>\n",
       "      <td>The problem with business culture versus start...</td>\n",
       "      <td>[Five Founders\\n\\nApril 2009\\n\\nInc recently a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What's the single biggest thing the government...</td>\n",
       "      <td>Establish a new class of visa for startup foun...</td>\n",
       "      <td>Establish a new class of visa for startup foun...</td>\n",
       "      <td>[Five Founders\\n\\nApril 2009\\n\\nInc recently a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0              What makes Sam Altman a good founder?   \n",
       "1        When was the essay \"Five Founders\" written?   \n",
       "2  When does the most dramatic growth happen for ...   \n",
       "3  What is the problem with business culture vers...   \n",
       "4  What's the single biggest thing the government...   \n",
       "\n",
       "                                    reference_answer  \\\n",
       "0                      He has a great force of will.   \n",
       "1                                         April 2009   \n",
       "2    When the startup only has three or four people.   \n",
       "3  In business culture, energy is expended on out...   \n",
       "4  Establish a new class of visa for startup foun...   \n",
       "\n",
       "                                          llm_answer  \\\n",
       "0  Sam Altman is considered a good founder becaus...   \n",
       "1                                         April 2009   \n",
       "2  The most dramatic growth for a startup typical...   \n",
       "3  The problem with business culture versus start...   \n",
       "4  Establish a new class of visa for startup foun...   \n",
       "\n",
       "                                   retrieved_context  answer_similarity  \\\n",
       "0  [Five Founders\\n\\nApril 2009\\n\\nInc recently a...                4.0   \n",
       "1  [Five Founders\\n\\nApril 2009\\n\\nInc recently a...                5.0   \n",
       "2  [Five Founders\\n\\nApril 2009\\n\\nInc recently a...                5.0   \n",
       "3  [Five Founders\\n\\nApril 2009\\n\\nInc recently a...                5.0   \n",
       "4  [Five Founders\\n\\nApril 2009\\n\\nInc recently a...                5.0   \n",
       "\n",
       "   retrieval_precision  augmentation_accuracy  augmentation_precision  \\\n",
       "0                  1.0                    1.0                     1.0   \n",
       "1                  0.5                    0.5                     1.0   \n",
       "2                  0.0                    0.0                     0.0   \n",
       "3                  0.0                    0.0                     0.0   \n",
       "4                  0.0                    0.0                     0.0   \n",
       "\n",
       "   answer_consistency  \n",
       "0                 0.8  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e264575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tvalmetrics_test_env)",
   "language": "python",
   "name": "tvalmetrics_test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
